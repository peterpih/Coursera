***Attributed: Stuart Ward (predmachlearn-031)***  

**Validation advice**: K-fold cross validation is especially beneficial when your data set is limited in size, but as the number of samples increases, then the size of K can be significantly decreased. Or in the case of large, dense, robust data sets, a simple random stratified hold-out validation set (70/30 split of training data) may be all that is necessary to train and validate a highly accurate model.

**Caret advice**: similar to the advice for validation, if your data set is small in size and scope, the defaults for caret can work quite well. As the data set increases in size and complexity then the caret package defaults can lead to unnecessarily long training times. To prevent this, you must finely control the settings, such as for cross validation and mtry (as an example for training a random forest model in caret). A simpler option might be to bypass the caret package and utilize the machine learning algorithm directly (such as the "randomForest" package).

**Model advice**: it has been suggested when approaching a new machine learning challenge, to first pick an algorithm that trains very quickly to set a benchmark to work from. For example, applied to a large, dense data set (say, one with tens of thousands of rows and dozens of columns), a QDA model can be trained in less than one second (using a machine with 1 core and 1 GB of RAM) with very high predictive accuracy. Once a benchmark has been set you can venture on to other models with traditionally higher accuracy rates (such as Random Forests), while knowing there will be trade-offs in training time (and as I referenced above, these time trade-offs should be in minutes, not hours or longer).

Even though this graphic (http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) was created on a synthetic data set, I find it helpful to visualize the decision boundaries and compare accuracy rates for a number of algorithms.
